# Names and values {#names-values}

```{r setup, include = FALSE}
source("common.R")
library(pryr)
library(rlang)
knitr::read_chunk("memory-read-delim.r")
options(scipen = 9)

id <- function() {
  x <- sample(c(0:9, letters[1:6]), 3, replace = TRUE)
  paste0("0x", paste(x, collapse = ""))
}
```

## Introduction

<!--
Names and values
https://nedbatchelder.com/text/names1.html

Immutable values
Reference semantics
garbage collection
size of aliased objects
modification in place
call-by-value vs call-by-reference

for(x in 1:10) {}
x
-->

Understanding the distinction between names and values is critical.

An improved mental will help you better predict performance and memory usage of R code. It can even help you write faster code because accidental copies are a major cause of slow code.

It will also help you better understand R's functional programming tools.

Along the way, you'll learn about some common myths, such as that you need to call `gc()` to free up memory, or that `for` loops are always slow. 

### Outline {-}

### Prerequisites {-}


### Sources {-}

<!-- FIXME: cite R-exts and R-ints formally -->

The details of R's memory management are not documented in a single place. Most of the information in this chapter was gleaned from a close reading of the documentation (particularly `?Memory` and `?gc`), the [memory profiling](http://cran.r-project.org/doc/manuals/R-exts.html#Profiling-R-code-for-memory-use) section of R-exts, and the [SEXPs](http://cran.r-project.org/doc/manuals/R-ints.html#SEXPs) section of R-ints. The rest I figured out by reading the C source code, performing small experiments, and asking questions on R-devel. Any mistakes are entirely mine.

## Overview
\index{bindings} \index{assignment}

Assignment is the act of binding (or rebinding) a name to a value. It is the counterpart to scoping, the set of rules that determines how to find the value associated with a name.  We'll start by exploring what happens when you use `<-` to create and modify bindings.

### Vectors

When you read code like `x <- 1:3` it's easy to think of it as creating an object called `x`. But actually there are two things going on:

* We create the object (`1:3`)
* Then __bind__ it to a name (`x`).

We'll draw that as:

```{r}
x <- 1:3
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/binding-1.png", dpi = 300)
```

Note that the arrow points in the opposite direction. Assignment creates the binding. We look up the value in the opposite direction.

Objects in R don't have names; names have objects. But it's difficult to talk about things that don't have names, so in diagrams we'll give objects labels like `0x6f4`. These are unique identifiers which look similar to the memory location of objects that we'll talk about later.

You can think of a name as a pointer. If you run code like `y <- x` we get two bindings to the same object:

```{r}
y <- x
```
```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/binding-2.png", dpi = 300)
```

What happens if we modify `x`? `y` doesn't also change

```{r}
y[[3]] <- 4
y
```

That's because most R objects are __immutable__. That means you can't change the object; instead you create a modified copy. `x` continues to reference the same object (`0x74b`), and `y` references a new object, `0xcd2`.

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/binding-3.png", dpi = 300)
```

### Functional calls

These same rules also apply to function calls. Take this function and simple set of calls:

```{r}
f <- function(a) {
  a
}
x <- 1:3
z <- f(x)
```

While `f()` is running, `a` inside its execution environment will point to the same value:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/binding-f1.png", dpi = 300)
```

And once complete, `z` will point to the same thing.

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/binding-f2.png", dpi = 300)
```

No copies were made because no modifications were made.

### Lists

It's not just top-level names (i.e. variables) that point to values. Lists do too. Take this list, which superficially is very similar to the vector above:

```{r}
l1 <- list(1, 2, 3)
```

But the internal representation of the list is actually quite different to that of a vector. A list is really a list of references:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/list.png", dpi = 300)
```

This is particularly important when we modify a list:

```{r}
l1 <- list(1, 2, 3)
l2 <- l1
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/l-modify-1.png", dpi = 300)
```

```{r}
l2[[3]] <- 4
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/l-modify-2.png", dpi = 300)
```

Like vectors, lists are immutable; the original list is left unchanged, but instead create a modified copy. Note that the copy is __shallow__: only the list object is copied (and modified), the individual elements are not.

### Dataframes

Data frames are lists, so this behaviour has important consequences when you modify a data frame. Let's take this data frame:

```{r}
d1 <- data.frame(x = c(1, 5, 6), y = c(2, 4, 3))
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/dataframe.png", dpi = 300)
```

If you modify a column, only that column needs to be modified; the others can continue to point to the same place.

```{r}
d2 <- d1
d2$y <- d2$y * 2
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/d-modify-c.png", dpi = 300)
```

However, if you modify a row, there is no way to share data with the previous version of the data frame.

```{r}
d3 <- d1
d3[1, ] <- d3[1, ] * 2
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/d-modify-r.png", dpi = 300)
```

### Environments {#env-modify}

There are two exceptions to the copy-on-modify rule. The first is environments, which you'll learn more in [Environments]. Environments have reference semantics so unlike vectors and lists they are modified in place.  Take this environment, which we reference from `e1` and `e2`:

```{r}
e1 <- env(a = 1, b = 2, c = 3)
e2 <- e1
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/e-modify-1.png", dpi = 300)
```

If we modify one reference, the environment is modified in place:

```{r}
e1$c <- 4
e2$d
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/e-modify-2.png", dpi = 300)
```

### Mononymous objects

There is one other exception to the copy-on-modify rule: objects that only have single reference pointing to them. I think it's best to think of this as a performance optimisation; if an object only has one name that points to it, then this has the same behaviour as copy-on-modify, but is obviously much faster.

```{r}
v <- 1:3
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/v-inplace-1.png", dpi = 300)
```

```{r}
v[3] <- 4L
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/v-inplace-2.png", dpi = 300)
```

The rules for when R applies this optimisation are a little subtle due to the way it counts the number of references that point to an object, so we'll come back to it the end of the chapter.

### Unbinding and the garbage collector

Consider the following sequence of function calls

```{r}
x <- 1:3
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/unbinding-1.png", dpi = 300)
```

```{r}
x <- 2:4
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/unbinding-2.png", dpi = 300)
```

```{r}
rm(x)
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/name-value/unbinding-3.png", dpi = 300)
```

Note that `rm()` doesn't delete the object; it deletes the binding.

So how do the objects get deleted? That's the job of the __garbage collector__. It walks through every object in memory and looks to see how many references it has. If there are no references, the object is deleted. We'll come back to this in more detail below.

## Assignment

### `<<-`

The regular assignment arrow, `<-`, always creates a variable in the current environment. The deep assignment arrow, `<<-`, never creates a variable in the current environment, but instead modifies an existing variable found by walking up the parent environments. 

```{r}
x <- 0
f <- function() {
  x <<- 1
}
f()
x
```

If `<<-` doesn't find an existing variable, it will create one in the global environment. This is usually undesirable, because global variables introduce non-obvious dependencies between functions. `<<-` is most often used in conjunction with a closure, as described in [Closures](#closures).


### Non-syntactic names
\index{reserved names} \indexc{`} \index{non-syntactic names}

You've probably used regular assignment in R thousands of times. Regular assignment creates a binding between a name and an object in the current environment. Names usually consist of letters, digits, `.` and `_`, and can't begin with `_`.  If you try to use a name that doesn't follow these rules, you get an error:

```{r, eval = FALSE}
_abc <- 1
# Error: unexpected input in "_"
```

Reserved words (like `TRUE`, `NULL`, `if`, and `function`) follow the rules but are reserved by R for other purposes:

```{r, eval = FALSE}
if <- 10
#> Error: unexpected assignment in "if <-"
```

A complete list of reserved words can be found in `?Reserved`. 

It's possible to override the usual rules and use a name with any sequence of characters by surrounding the name with backticks:

```{r, eval = FALSE}
`a + b` <- 3
`:)` <- "smile"
`    ` <- "spaces"
ls()
#  [1] "    "   ":)"     "a + b"
`:)`
#  [1] "smile"
```

::: sidebar
### Quotes {-}
You can also create non-syntactic bindings using single and double quotes instead of backticks, but I don't recommend it. The ability to use strings on the left hand side of the assignment arrow is a historical artefact, used before R supported backticks.
:::

## Lexical scoping {#lexical-scoping}

Assignment is the act of binding a name to a value. Scoping is the opposite; finding a value given a name.

Scoping is the set of rules that govern how R looks up the value of a symbol. In the example below, scoping is the set of rules that R applies to go from the symbol `x` to its value `10`: \index{scoping!lexical|see{lexical scoping}} \index{lexical scoping}

```{r}
x <- 10
x
```

Understanding scoping allows you to:

* build tools by composing functions, as described in 
  [functional programming](#functional-programming).

* overrule the usual evaluation rules and do non-standard evaluation, as 
  described in [non-standard evaluation](#nse).

R has two types of scoping: __lexical scoping__, implemented automatically at the language level, and __dynamic scoping__, used in select functions to save typing during interactive analysis. We discuss lexical scoping here because it is intimately tied to function creation. Dynamic scoping is described in more detail in [scoping issues](#scoping-issues).

Lexical scoping looks up symbol values based on how functions were nested when they were created, not how they are nested when they are called. With lexical scoping, you don't need to know how the function is called to figure out where the value of a variable will be looked up. You just need to look at the function's definition.

The "lexical" in lexical scoping doesn't correspond to the usual English definition ("of or relating to words or the vocabulary of a language as distinguished from its grammar and construction") but comes from the computer science term "lexing", which is part of the process that converts code represented as text to meaningful pieces that the programming language understands.

There are four basic principles behind R's implementation of lexical scoping:

* name masking
* functions vs. variables
* a fresh start
* dynamic lookup

You probably know many of these principles already, although you might not have thought about them explicitly. Test your knowledge by mentally running through the code in each block before looking at the answers.

### Name masking

The following example illustrates the most basic principle of lexical scoping, and you should have no problem predicting the output.

```{r, eval = FALSE}
f <- function() {
  x <- 1
  y <- 2
  c(x, y)
}
f()
rm(f)
```

If a name isn't defined inside a function, R will look one level up.

```{r, eval = FALSE}
x <- 2
g <- function() {
  y <- 1
  c(x, y)
}
g()
rm(x, g)
```

The same rules apply if a function is defined inside another function: look inside the current function, then where that function was defined, and so on, all the way up to the global environment, and then on to other loaded packages. Run the following code in your head, then confirm the output by running the R code.

```{r, eval = FALSE}
x <- 1
h <- function() {
  y <- 2
  i <- function() {
    z <- 3
    c(x, y, z)
  }
  i()
}
h()
rm(x, h)
```

The same rules apply to closures, functions created by other functions. Closures will be described in more detail in [functional programming](#functional-programming); here we'll just look at how they interact with scoping. The following function, `j()`, returns a function.  What do you think this function will return when we call it? \index{closures!scoping}

```{r, eval = FALSE}
j <- function(x) {
  y <- 2
  function() {
    c(x, y)
  }
}
k <- j(1)
k()
rm(j, k)
```

This seems a little magical (how does R know what the value of `y` is after the function has been called). It works because `k` preserves the environment in which it was defined and because the environment includes the value of `y`. [Environments](#environments) gives some pointers on how you can dive in and figure out what values are stored in the environment associated with each function.

### Functions vs. variables

The same principles apply regardless of the type of associated value --- finding functions works exactly the same way as finding variables:

```{r}
l <- function(x) x + 1
m <- function() {
  l <- function(x) x * 2
  l(10)
}
m()
rm(l, m)
```

For functions, there is one small tweak to the rule. If you are using a name in a context where it's obvious that you want a function (e.g., `f(3)`), R will ignore objects that are not functions while it is searching. In the following example `n` takes on a different value depending on whether R is looking for a function or a variable.

```{r}
n <- function(x) x / 2
o <- function() {
  n <- 10
  n(n)
}
o()
rm(n, o)
```

However, using the same name for functions and other objects will make for confusing code, and is generally best avoided.

### A fresh start {#fresh-start}

What happens to the values in between invocations of a function? What will happen the first time you run this function? What will happen the second time? (If you haven't seen `exists()` before: it returns `TRUE` if there's a variable of that name, otherwise it returns `FALSE`.)

```{r, eval = FALSE}
j <- function() {
  if (!exists("a")) {
    a <- 1
  } else {
    a <- a + 1
  }
  a
}
j()
rm(j)
```

You might be surprised that it returns the same value, `1`, every time. This is because every time a function is called, a new environment is created to host execution. A function has no way to tell what happened the last time it was run; each invocation is completely independent. (We'll see some ways to get around this in [mutable state](#mutable-state).)

### Dynamic lookup

Lexical scoping determines where to look for values, not when to look for them. R looks for values when the function is run, not when it's created. This means that the output of a function can be different depending on objects outside its environment: 

```{r}
f <- function() x
x <- 15
f()

x <- 20
f()
```

You generally want to avoid this behaviour because it means the function is no longer self-contained. This is a common error --- if you make a spelling mistake in your code, you won't get an error when you create the function, and you might not even get one when you run the function, depending on what variables are defined in the global environment.

One way to detect this problem is the `findGlobals()` function from `codetools`. This function lists all the external dependencies of a function: \indexc{findGlobals()}

```{r}
f <- function() x + 1
codetools::findGlobals(f)
```

Another way to try and solve the problem would be to manually change the environment of the function to the `emptyenv()`, an environment which contains absolutely nothing:

```{r, error = TRUE}
environment(f) <- emptyenv()
f()
```

This doesn't work because R relies on lexical scoping to find _everything_, even the `+` operator. It's never possible to make a function completely self-contained because you must always rely on functions defined in base R or other packages.

You can use this same idea to do other things that are extremely ill-advised. For example, since all of the standard operators in R are functions, you can override them with your own alternatives.  If you ever are feeling particularly evil, run the following code while your friend is away from their computer:

```{r}
`(` <- function(e1) {
  if (is.numeric(e1) && runif(1) < 0.1) {
    e1 + 1
  } else {
    e1
  }
}
replicate(50, (1 + 2))
rm("(")
```

This will introduce a particularly pernicious bug: 10% of the time, 1 will be added to any numeric calculation inside parentheses. This is another good reason to regularly restart with a clean R session!

### Exercises

1. What does the following code return? Why? What does each of the three `c`'s mean?

    ```{r, eval = FALSE}
    c <- 10
    c(c = c)
    ```

2. What are the four principles that govern how R looks for values?

3. What does the following function return? Make a prediction before 
   running the code yourself.

    ```{r, eval = FALSE}
    f <- function(x) {
      f <- function(x) {
        f <- function(x) {
          x ^ 2
        }
        f(x) + 1
      }
      f(x) * 2
    }
    f(10)
    ```


## Object size {#object-size}

<!-- deletion size vs. serialization size -->

To understand memory usage in R, we will start with `pryr::object_size()`. This function tells you how many bytes of memory an object occupies: \index{object\_size()}

```{r}
library(pryr)
object_size(1:10)
object_size(mean)
object_size(mtcars)
```

(This function is better than the built-in `object.size()` because it accounts for shared elements within an object and includes the size of environments.)

Something interesting occurs if we use `object_size()` to systematically explore the size of an integer vector. The code below computes and plots the memory usage of integer vectors ranging in length from 0 to 50 elements. You might expect that the size of an empty vector would be zero and that memory usage would grow proportionately with length. Neither of those things are true! \index{vectors!size of}

```{r size-q, fig.height = 2.5, fig.width = 4, small_mar = TRUE}
sizes <- sapply(0:50, function(n) object_size(seq_len(n)))
plot(0:50, sizes, xlab = "Length", ylab = "Size (bytes)", 
  type = "s")
```

This isn't just an artefact of integer vectors. Every length 0 vector occupies 40 bytes of memory:

```{r}
object_size(numeric())
object_size(logical())
object_size(raw())
object_size(list())
```

Those 40 bytes are used to store four components possessed by every object in R:

* Object metadata (4 bytes). These metadata store the base type (e.g. integer) 
  and information used for debugging and memory management.

* Two pointers: one to the next object in memory and one to the previous 
  object (2 * 8 bytes). This doubly-linked list makes it easy for internal 
  R code to loop through every object in memory.

* A pointer to the attributes (8 bytes).

All vectors have three additional components: \indexc{SEXP}

* The length of the vector (4 bytes). By using only 4 bytes, you might expect 
  that R could only support vectors up to $2 ^ {4 \times 8 - 1}$ ($2 ^ {31}$, about 
  two billion) elements. But in R 3.0.0 and later, you can actually have 
  vectors up to $2 ^ {52}$ elements. [Read R-internals][long-vectors] to see how 
  support for long vectors was added without having to change the size of this 
  field. \index{long vectors} \index{atomic vectors!long}

* The "true" length of the vector (4 bytes). This is basically never used, 
  except when the object is the hash table used for an environment. In that 
  case, the true length represents the allocated space, and the length 
  represents the space currently used.

* The data (variable number of bytes). An empty vector has 0 bytes of data. Numeric vectors occupy 8 bytes for
  every element, integer vectors 4, and complex vectors 16.

If you're keeping count you'll notice that this only adds up to 36 bytes. The remaining 4 bytes are used for padding so that each component starts on an 8 byte (= 64-bit) boundary. Most cpu architectures require pointers to be aligned in this way, and even if they don't require it, accessing non-aligned pointers tends to be rather slow. (If you're interested, you can read more about it in [C structure packing](http://www.catb.org/esr/structure-packing/).)

This explains the intercept on the graph. But why does the memory size grow irregularly? To understand why, you need to know a little bit about how R requests memory from the operating system. Requesting memory (with `malloc()`) is a relatively expensive operation. Having to request memory every time a small vector is created would slow R down considerably. Instead, R asks for a big block of memory and then manages that block itself. This block is called the small vector pool and is used for vectors less than 128 bytes long. For efficiency and simplicity, it only allocates vectors that are 8, 16, 32, 48, 64, or 128 bytes long. If we adjust our previous plot to remove the 40 bytes of overhead, we can see that those values correspond to the jumps in memory use.

```{r size-a, fig.height = 2.5, fig.width = 4, small_mar = TRUE}
plot(0:50, sizes - 40, xlab = "Length", 
  ylab = "Bytes excluding overhead", type = "n")
abline(h = 0, col = "grey80")
abline(h = c(8, 16, 32, 48, 64, 128), col = "grey80")
abline(a = 0, b = 4, col = "grey90", lwd = 4)
lines(sizes - 40, type = "s")
```

Beyond 128 bytes, it no longer makes sense for R to manage vectors. After all, allocating big chunks of memory is something that operating systems are very good at. Beyond 128 bytes, R will ask for memory in multiples of 8 bytes. This ensures good alignment.

A subtlety of the size of an object is that components can be shared across multiple objects. For example, look at the following code:

```{r}
x <- 1:1e6
object_size(x)

y <- list(x, x, x)
object_size(y)
```

`y` isn't three times as big as `x` because R is smart enough to not copy `x` three times; instead it just points to the existing `x`. 

It's misleading to look at the sizes of `x` and `y` individually. If you want to know how much space they take up together, you have to supply them to the same `object_size()` call:

```{r}
object_size(x, y)
```

In this case, `x` and `y` together take up the same amount of space as `y` alone. This is not always the case. If there are no shared components, as in the following example, then you can add up the sizes of individual components to find out the total size:

```{r}
x1 <- 1:1e6
y1 <- list(1:1e6, 1:1e6, 1:1e6)

object_size(x1)
object_size(y1)
object_size(x1, y1)
object_size(x1) + object_size(y1) == object_size(x1, y1)
```

The same issue also comes up with strings, because R has a global string pool. This means that each unique string is only stored in one place, and therefore character vectors take up less memory than you might expect: \index{string pool}

```{r}
object_size("banana")
object_size(rep("banana", 10))
```

### Exercises

1.  Repeat the analysis above for numeric, logical, and complex vectors.

1.  If a data frame has one million rows, and three variables (two numeric, and 
    one integer), how much space will it take up? Work it out from theory, 
    then verify your work by creating a data frame and measuring its size.

1.  Compare the sizes of the elements in the following two lists. Each 
    contains basically the same data, but one contains vectors of small 
    strings while the other contains a single long string.

    ```{r}
    vec <- lapply(0:50, function(i) c("ba", rep("na", i)))
    str <- lapply(vec, paste0, collapse = "")
    ```

1.  Which takes up more memory: a factor (`x`) or the equivalent character 
    vector (`as.character(x)`)? Why?

1.  Explain the difference in size between `1:5` and `list(1:5)`.

## Memory usage and garbage collection {#gc}

While `object_size()` tells you the size of a single object, `pryr::mem_used()` tells you the total size of all objects in memory: \indexc{mem\_used()}

```{r}
library(pryr)
mem_used()
```

This number won't agree with the amount of memory reported by your operating system for a number of reasons:

1. It only includes objects created by R, not the R interpreter itself.

1. Both R and the operating system are lazy: they won't reclaim memory 
   until it's actually needed. R might be holding on to memory because 
   the OS hasn't yet asked for it back.

1. R counts the memory occupied by objects but there may be gaps due to 
   deleted objects. This problem is known as memory fragmentation.

`mem_change()` builds on top of `mem_used()` to tell you how memory changes during code execution. Positive numbers represent an increase in the memory used by R, and negative numbers represent a decrease. \indexc{mem\_change()}

```{r}
# Need about 4 mb to store 1 million integers
mem_change(x <- 1:1e6)
# We get that memory back when we delete it
mem_change(rm(x))
```

Even operations that don't do anything use up a little memory. This is because R is tracking the history of everything you do. You can ignore anything smaller than a couple kB.

```{r}
mem_change(NULL)
mem_change(NULL)
```

In some languages, you have to explicitly delete unused objects for their memory to be returned. R uses an alternative approach: garbage collection (or GC for short). GC automatically releases memory when an object is no longer used. It does this by tracking how many names point to each object, and when there are no names pointing to an object, it deletes that object. \index{garbage collection}

```{r, echo = FALSE}
rm(y)
```

```{r}
# Create a big object
mem_change(x <- 1:1e6)
# Also point to 1:1e6 from y
mem_change(y <- x)
# Remove x, no memory freed because y is still pointing to it
mem_change(rm(x))
# Now nothing points to it and the memory can be freed
mem_change(rm(y))
```

Despite what you might have read elsewhere, there's never any need to call `gc()` yourself. R will automatically run garbage collection whenever it needs more space; if you want to see when that is, call `gcinfo(TRUE)`. The only reason you _might_ want to call `gc()` is to ask R to return memory to the operating system. However, even that might not have any effect: older versions of Windows had no way for a program to return memory to the OS. \indexc{gc()}

GC takes care of releasing objects that are no longer used. However, you do need to be aware of possible memory leaks. A memory leak occurs when you keep pointing to an object without realising it. In R, the two main causes of memory leaks are formulas and closures because they both capture the enclosing environment. The following code illustrates the problem. In `f1()`, `1:1e6` is only referenced inside the function, so when the function completes the memory is returned and the net memory change is 0. `f2()` and `f3()` both return objects that capture environments, so that `x` is not freed when the function completes. \index{memory!leaks}

```{r}
f1 <- function() {
  x <- 1:1e6
  10
}
mem_change(x <- f1())
object_size(x)

f2 <- function() {
  x <- 1:1e6
  a ~ b
}
mem_change(y <- f2())
object_size(y)

f3 <- function() {
  x <- 1:1e6
  function() 10
}
mem_change(z <- f3())
object_size(z)
```

```{r, echo = FALSE}
rm(y, z)
```


## Modification in place {#modification}

What happens to `x` in the following code? \index{copy-on-modify!exceptions} \index{avoiding copies}

```{r}
x <- 1:10
x[5] <- 10L
x
```

There are two possibilities:

1. R modifies `x` in place.

2. R makes a copy of `x` to a new location, modifies the copy, and then uses 
   the name `x` to point to the new location.

It turns out that R can do either depending on the circumstances. In the example above, it will modify in place. But if another variable also points to `x`, then R will copy it to a new location. To explore what's going on in greater detail, we use two tools from the pryr package. Given the name of a variable, `address()` will tell us the variable's location in memory and `refs()` will tell us how many names point to that location. \indexc{address()} \indexc{refs()}

```{r, eval = FALSE}
library(pryr)
x <- 1:10
c(address(x), refs(x))
# [1] "0x103100060" "1"

y <- x
c(address(y), refs(y))
# [1] "0x103100060" "2"
```

(Note that if you're using RStudio, `refs()` will always return 2: the environment browser makes a reference to every object you create on the command line.)

`refs()` is only an estimate. It can only distinguish between one and more than one reference (future versions of R might do better). This means that `refs()` returns 2 in both of the following cases: \index{reference counting}

```{r}
x <- 1:5
y <- x
rm(y)
# Should really be 1, because we've deleted y
refs(x)

x <- 1:5
y <- x
z <- x
# Should really be 3
refs(x)
```

When `refs(x)` is 1, modification will occur in place. When `refs(x)` is 2, R will make a copy (this ensures that other pointers to the object remain unaffected). Note that in the following example, `y` keeps pointing to the same location while `x` changes.

```{r}
x <- 1:10
y <- x
c(address(x), address(y))

x[5] <- 6L
c(address(x), address(y))
```

Another useful function is `tracemem()`. It prints a message every time the traced object is copied: \indexc{tracemem()}

```{r tracemem, eval = FALSE}
x <- 1:10
# Prints the current memory location of the object
tracemem(x)
# [1] "<0x7feeaaa1c6b8>"

x[5] <- 6L

y <- x
# Prints where it has moved from and to
x[5] <- 6L
# tracemem[0x7feeaaa1c6b8 -> 0x7feeaaa1c768]:
```

For interactive use, `tracemem()` is slightly more useful than `refs()`, but because it just prints a message, it's harder to program with. I don't use it in this book because it interacts poorly with [knitr](http://yihui.name/knitr/), the tool I use to interleave text and code.

Non-primitive functions that touch the object always increment the ref count. Primitive functions usually don't. (The reasons are a little complicated, but see the R-devel thread [confused about NAMED](http://r.789695.n4.nabble.com/Confused-about-NAMED-td4103326.html).) \index{primitive functions}

```{r}
# Touching the object forces an increment
f <- function(x) x
{x <- 1:10; f(x); refs(x)}

# Sum is primitive, so no increment
{x <- 1:10; sum(x); refs(x)}

# f() and g() never evaluate x, so refs don't increment
f <- function(x) 10
g <- function(x) substitute(x)

{x <- 1:10; f(x); refs(x)}
{x <- 1:10; g(x); refs(x)}
```

Generally, provided that the object is not referred to elsewhere, any primitive replacement function will modify in place. This includes `[[<-`, `[<-`, `@<-`, `$<-`, `attr<-`, `attributes<-`, `class<-`, `dim<-`, `dimnames<-`, `names<-`, and `levels<-`. To be precise, all non-primitive functions increment refs, but a primitive function may be written in such a way that it doesn't. The rules are sufficiently complicated that there's little point in trying to memorise them. Instead, you should approach the problem practically by using `refs()` and `address()` to figure out when objects are being copied. \index{subsetting|subassignment}

While determining that copies are being made is not hard, preventing such behaviour is. If you find yourself resorting to exotic tricks to avoid copies, it may be time to rewrite your function in C++, as described in [Rcpp](#rcpp).

### Loops

For loops in R have a reputation for being slow. Often that slowness is because you're modifying a copy instead of modifying in place. Consider the following code. It subtracts the median from each column of a large data frame: \index{loops!avoiding copies}

```{r, cache = TRUE}
x <- data.frame(matrix(runif(100 * 1e4), ncol = 100))
medians <- vapply(x, median, numeric(1))

for (i in seq_along(medians)) {
  x[[i]] <- x[[i]] - medians[[i]]
}
```

You may be surprised to realise that every iteration of the loop copies the data frame. We can see that more clearly by using `address()` and `refs()` for a small sample of the loop:

```{r}
for (i in 1:10) {
  x[[i]] <- x[[i]] - medians[[i]]
  print(c(address(x), refs(x)))
}
```

For each iteration, `x` is moved to a new location so `refs(x)` is always 2. This occurs because `[[<-.data.frame` is not a primitive function, so it always increments the refs. We can make the function substantially more efficient by using a list instead of a data frame. Modifying a list uses primitive functions, so the refs are not incremented and all modifications occur in place:

```{r}
y <- as.list(x)

for (i in 1:10) {
  y[[i]] <- y[[i]] - medians[[i]]
  print(c(address(y), refs(y)))
}
```

### Exercises

1.  The code below makes one duplication. Where does it occur and why? 
    (Hint: look at `refs(y)`.)

    ```{r}
    y <- as.list(x)
    for(i in seq_along(medians)) {
      y[[i]] <- y[[i]] - medians[[i]]
    }
    ```

1.  The implementation of `as.data.frame()` in the previous section has one 
    big downside. What is it and how could you avoid it?


[long-vectors]: http://cran.r-project.org/doc/manuals/R-ints.html#Long-vectors
