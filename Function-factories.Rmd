# Function factories
\index{function factories}

```{r, include = FALSE}
source("common.R")
```

## Introduction

A __function factory__ is a function that creates functions. Here's a very simple example: we use a function factory (`power1()`) to make two child functions (`square()` and `cube()`):

```{r}
power1 <- function(exp) {
  force(exp)
  
  function(x) {
    x ^ exp
  }
}

square <- power1(2)
cube <- power1(3)
```

I'll call `square()` and `cube()` __manufactured functions__, but this is just a term to ease communication with other humans. From R's perspective they are no different to functions created any other way:

```{r}
square(3)
cube(3)
```

You have already learned about the individual components that make function factories possible:

* In Section \@ref(first-class-functions), you learned about R's "first class" 
  functions and how `<-` binds a function to a name no differently to any other
  type of object.

* In Section \@ref(the-function-environment), you learned that a function
  captures, or encloses, the environment it which it is created.

* In Section \@ref(execution-environments), you learned that a function 
  creates a new execution environment every time it is run. This environment
  is usually ephemeral, but here it becomes the enclosing environment of 
  the manufactured function.

In this chapter, you'll learn about the non-obvious combination of these three features that lead to the function factory. Of the three main FP tools, function factories are probably the least useful. Generally, they don't tend to reduce overall code complexity. Instead, they provide a toolkit for breaking down a complex problem into smaller pieces that can be considered separately. They are also an important building block for the very useful function operators, which you'll learn about in the next chapter.

### Outline {-}

* Section \@ref(factory-fundamentals) begins the chapter with a description
  of how function factories work, pulling together ideas from scoping and 
  environments. You'll also see how function factories can be used to implement
  a "memory" for functions, allowing data to persist across calls.

* Section \@ref(graph-fact) motivates the use of function factories with 
  examples from ggplot2. You'll see two examples of how ggplot2 works
  with user supplied function factories, and one example of where ggplot2 
  uses a function factory internally.

* Section \@ref(stat-fact) uses function factories for three statistical 
  challenges: the Box-Cox transformation, maximum likelihood estimation, and
  boostrapping. Here you'll see how function factories can partition problems, 
  doing some work up front in order to save time in the long-run.

* Section \@ref(functional-factories) shows how you can use function factories 
  with functionals to rapidly generate a family of functions from data.

### Prerequisites {-}

Make sure you're familiar with the contents of Sections \@ref(first-class-functions), \@ref(the-function-environment), and \@ref(execution-environments) mentioned above.

Function factories rely on base R functionality, but we'll use a little rlang to peek inside of them more easily. We'll also use ggplot2 and scales to explore the function factories in visualisation.

```{r setup}
# The development version includes some printing tweaks that we need here
# devtools::install_github("r-lib/rlang")
library(rlang)

library(ggplot2)
library(scales)
```

## Factory fundamentals
\index{closures|see{functions}}

The key idea that makes function factories work can be expressed very concisely:

> The enclosing environment of the manufactured function is the execution
> environment of the function factory. 

It only takes few words to express these big ideas, but it takes a lot more work to really understand what this means. This section will help you put the pieces together with interactive exploration and some diagrams.

### Environments

Let's start by taking a look at `square()` and `cube()`:

```{r}
square

cube
```

Printing manufactured functions is not revealing because the bodies are identical, and the contents of the environments are not printed. We can get a little more insight by using `rlang::env_print()`. That shows us that we have two different environments (each of which was originally an execution environment of `power()`). The environments have the same parent, which is the enclosing environment of `power1()`, the global environment.

```{r}
env_print(square)

env_print(cube)
```

While `env_print()` shows us that both environments have a binding to `exp`, to see key difference, we need to extract the value of `exp` (a future version of `env_print()` is likely to do better at summarising the contents so you don't need this step). That's easily done with `env_get()`:

```{r}
env_get(square, "exp")

env_get(cube, "exp")
```

### Diagram conventions

We can also show these relationships in a diagram:

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/function-factories/power-full.png", dpi = 300)
```

There's a lot going on this diagram and some of the details aren't that important. We can simplify considerably by using two conventions:

* Any free floating symbol lives in the global environment.

* Any environment without an explicit parent inherits from the global 
  environment.

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/function-factories/power-simple.png", dpi = 300)
```

This view, which focuses on the environments, doesn't show any direct link between `cube()` and `square()`. That's because the link is the through the body of the function, which is identical for both, but is not shown in this diagram.

### Statefun functions {#stateful-funs}
\indexc{<<-} 
\index{copy-on-modify!exceptions}

Function factories also allow you to maintain state across function invocations. This is possible because while the execution environment is refreshed every time, the enclosing environment is constant. The key to managing variables at different levels is the double arrow assignment operator (`<<-`). Unlike the usual single arrow assignment (`<-`) that always assigns in the current environment, the double arrow operator will keep looking up the chain of parent environments until it finds a existing binding, and will modify that.

The following example shows a counter that records how many times a function has been called. Each time `new_counter` is run, it creates an environment, initialises the counter `i` in this environment, and then creates a new function.

```{r}
new_counter <- function() {
  i <- 0
  
  function() {
    i <<- i + 1
    i
  }
}

counter_one <- new_counter()
counter_two <- new_counter()
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/function-factories/counter-1.png", dpi = 300)
```

When the manufactured function is run, `i <<- i + 1` will modify `i` in the parent environment, the enclosing environment of the manufactured environment and an execution environment of the function factory. Because manufactured functions have independent enclosing environments, they have independent counts:

```{r}
counter_one()
counter_one()
counter_two()
```

```{r, echo = FALSE, out.width = NULL}
knitr::include_graphics("diagrams/function-factories/counter-2.png", dpi = 300)
```

Stateful functions are best used in moderation. As soon as your function starts managing the state of multiple variables, it's better to switch to R6, the topic of Chapter \@ref(R6).

### Potential pitfalls

There are two potentail pitfalls you need to be aware of when creating your own function factories: lazy evaluation and accidental capture of large objects. 

If you don't eagerly evaluate every argument to a function factory, it's possible to get confusing behaviour:

```{r}
power2 <- function(exp) {
  function(x) {
    x ^ exp
  }
}

exp2 <- 2
square2 <- power2(exp2)
exp2 <- 3

square2(2)
```

This is described in \@ref(forcing-evaluation), and happens when a binding changes in between calling the factory function and calling the manufactured function. This is likely to only happen rarely, but will lead to a real head-scratcher of a bug. Avoid future pain by ensuring every argument is evaluated, using `force()` where needed.

You usually don't worry about creating large temporary objects inside a function because the execution environment is ephemeral, and all objects are unbound when function completes. However, manufactured functions hold on to the execution environment, so you'll need to explicitly unbind any large temporary objects with `rm()`. Compare the sizes of `g1()` and `g2()` in the example below:

```{r}
f1 <- function(n) {
  x <- runif(n)
  m <- mean(x)
  function() m
}

g1 <- f1(1e6)
lobstr::obj_size(g1)

f2 <- function(n) {
  x <- runif(n)
  m <- mean(x)
  rm(x)
  function() m
}

g2 <- f2(1e6)
lobstr::obj_size(g2)
```

### Exercises

1.  Base R contains two function factories, `approxfun()` and `ecdf()`. 
    Read their documentation and experiment to figure out what the functions 
    do and what they return.

1.  Create a function `pick()` that takes an index, `i`, as an argument and 
    returns a function with an argument `x` that subsets `x` with `i`.

    ```{r, eval = FALSE}
    pick(1)(x)
    # should be equivalent to
    x[[1]]
    
    lapply(mtcars, pick(5))
    # should be equivalent to
    lapply(mtcars, function(x) x[[5]])
    ```

1.  What happens if you don't use a closure? Make predictions then verify with 
    the code below.

    ```{r}
    i <- 0
    new_counter2 <- function() {
      i <<- i + 1
      i
    }
    ```

1.  What happens if you use `<-` instead of `<<-`? Make predictions then verify 
    with the code below.

    ```{r}
    new_counter3 <- function() {
      i <- 0
      function() {
        i <- i + 1
        i
      }
    }
    ```

## Graphical factories {#graph-fact}

We'll begin our exploration of useful function factories with a few examples from ggplot2. 

### Labelling

The one of the goals of the [scales](http://scales.r-lib.org) package is to make it easy to customise the labels on ggplot2. 

At first glance the design of the scales package is a little odd. All the format functions[^suffix] return a function, and you then have to call that functon to format a number:

[^suffix]: It's an unfortunate accident of history that scales uses function suffixes instead of function prefixes. That's because it was written before I understood the autocomplete advantages to using common prefixes instead of common suffixes.

```{r}
y <- c(12345, 123456, 1234567)
comma_format()(y)

number_format(scale = 1e-3, suffix = " K")(y)
```

In other words, the primary interface of scales is function factories. At first glance, this seems like this adds extra complexity for little gain. But this makes for a nice interface when used with ggplot2 scales, because they accept functions in the `label` argument:

```{r, out.width = "25%", fig.align="default", fig.width = 2, fig.height = 4, fig.asp = NULL}
df <- data.frame(x = 1, y = y)
base <- ggplot(df, aes(x, y)) + 
  geom_point() + 
  scale_x_continuous(breaks = 1, labels = NULL) +
  labs(x = NULL, y = NULL)
  
base
base + scale_y_continuous(label = comma_format())
base + scale_y_continuous(label = number_format(scale = 1e-3, suffix = " K"))
base + scale_y_continuous(label = scientific_format())
```

The use of a function factory here allows for a flexible specification. The `xyz_format()` functions create an "object" that knows how to format a vector of breaks.

### Histogram bins

A little known feature of `geom_histogram()` is that the `binwidth` argument can be a function. This is particularly useful because the function is executed once for each group, which means you can have different binwidths in different facets, which is otherwise not possible.

To illustrate this idea, and see where variable binwidth might be useful, I'm going to construct an example where a fixed binwidth isn't great.

```{r, fig.width = 6, fig.height = 2.5, fig.asp = NULL, out.width = "90%"}
# construct some sample data with very different numbers in each cell
sd <- c(1, 5, 10)
n <- 100

df <- data.frame(x = rnorm(3 * n, sd = sd), sd = rep(sd, n))

ggplot(df, aes(x)) + 
  geom_histogram(binwidth = 2) + 
  facet_wrap(~ sd, scales = "free_x") + 
  labs(x = NULL)
```

Here we have the same amount of data in each facet, but very different spreads. It would be nice if we could request that the binwidth vary. We can do that by constructing a function factory. Our factory will take the number of bins as a parameter, and create a function that gives takes a numeric vector and returns a binwidth:

```{r, fig.width = 6, fig.height = 2.5, fig.asp = NULL, out.width = "90%"}
binwidth_bins <- function(n) {
  force(n)
  
  function(x) {
    (max(x) - min(x)) / n
  }
}

ggplot(df, aes(x)) + 
  geom_histogram(binwidth = binwidth_bins(20)) + 
  facet_wrap(~ sd, scales = "free_x") + 
  labs(x = NULL)
```

We could use this same pattern to wrap around the base R functions that automatically find the "optimal"[^optimal] binwidth:

```{r, fig.width = 6, fig.height = 2.5, fig.asp = NULL, out.width = "90%"}
base_bins <- function(type) {
  fun <- switch(type,
    Sturges = nclass.Sturges,
    scott = nclass.scott,
    FD = nclass.FD,
    stop("Unknown type", call. = FALSE)
  )
  
  function(x) {
    (max(x) - min(x)) / fun(x)
  }
}

ggplot(df, aes(x)) + 
  geom_histogram(binwidth = base_bins("FD")) + 
  facet_wrap(~ sd, scales = "free_x") + 
  labs(x = NULL)
```

[^optimal]: ggplot2 doesn't expose these functions directly because I don't think the defintion of definition of optimality needed to make the problem mathematically tractable is a good match to the actual needs of data exploration.

### ggsave

Finally, I want to show a function factory used internally by ggplot2. `ggplot2:::plot_dev()` is used by `ggsave()` to go from a file extension (e.g. `png`, `jpeg` etc) to a graphics device. There challenge here arises because the base graphics devices have some minor inconsistencies which we need to paper over:

* Most have `filename` as first argument but some have `file`.

* The `width` of `height` of raster graphic devices use pixels units
  by default, but the vector graphics use inches.

A mildly simplified version of `plot_dev()` is shown below:

```{r}
plot_dev <- function(ext, dpi = 96) {
  force(dpi)
  
  switch(ext,
    eps =  ,
    ps  =  function(filename, ...) {
      grDevices::postscript(
        file = filename, ..., onefile = FALSE, 
        horizontal = FALSE, paper = "special"
      )
    },
    tex =  function(filename, ...) grDevices::pictex(file = filename, ...),
    pdf =  function(filename, ...) grDevices::pdf(file = filename, ...),
    svg  = function(filename, ...) svglite::svglite(file = filename, ...),
    emf  = ,
    wmf  = function(...) grDevices::win.metafile(...),
    png  = function(...) grDevices::png(..., res = dpi, units = "in"),
    jpg  = ,
    jpeg = function(...) grDevices::jpeg(..., res = dpi, units = "in"),
    bmp  = function(...) grDevices::bmp(..., res = dpi, units = "in"),
    tiff = function(...) grDevices::tiff(..., res = dpi, units = "in"),
    stop("Unknown graphics extension: ", ext, call. = FALSE)
  )
}

plot_dev("pdf")
plot_dev("png")
```

### Exercises

1.  Compare and constrast `ggplot2::label_bquote()` with 
    `scales::number_format()`

## Statistical factories {#stat-fact}

Our first motivating examples for function factories come from statistics:

* The Box-Cox transformation
* Boostrapping 
* Maximum likelihood estimation. 

All these examples can be tackled without function factories, but I think they're a good fit for these problems and admit elegant solutions. These examples expect some statistical background, so feel free to skip if they don't make much sense to you.

### Box-Cox transformation
\index{Box-Cox transformation}

The Box-Cox transformation is a flexible transformation often used to transform  data towards normality. It has a single parameter, $\lambda$ which controls the strength of the transformation. We could express the transformation as simple two argument function:

```{r}
boxcox1 <- function(x, lambda) {
  stopifnot(length(lambda) != 1)
  
  if (lambda == 0) {
    log(x)
  } else {
    (x ^ lambda - 1) / lambda
  }
}
```

But re-formulating as a function factory makes it easy to explore its behaviour with `stat_function()`:

```{r}
boxcox2 <- function(lambda) {
  if (lambda == 0) {
    function(x) log(x)
  } else {
    function(x) (x ^ lambda - 1) / lambda
  }
}

ggplot(data.frame(x = c(0, 5)), aes(x)) + 
  stat_function(aes(colour = 2), fun = boxcox2(2), size = 1) +
  stat_function(aes(colour = 1), fun = boxcox2(1), size = 1) + 
  stat_function(aes(colour = 0.5), fun = boxcox2(0.5), size = 1) +
  scale_colour_viridis_c()

# visually, log() does seem to make sense as the limit as lambda -> 0
ggplot(data.frame(x = c(0.01, 1)), aes(x)) + 
  stat_function(aes(colour = 0.5), fun = boxcox2(0.5), size = 1) + 
  stat_function(aes(colour = 0.25), fun = boxcox2(0.25), size = 1) +
  stat_function(aes(colour = 0.1), fun = boxcox2(0.1), size = 1) +
  stat_function(aes(colour = 0), fun = boxcox2(0), size = 1) +
  scale_colour_viridis_c()
```

In general, this allows you to use a Box-Cox transformation with any function that accepts a unary transformation function: you don't have to worry about that function providing `...` to pass along additional arguments. I also think that the partitioning of `lambda` and `x` into two different function arguments is natural since `lambda` plays quite a different role to `x`. 

### Bootstrap generators
\index{boostraping generator}

Function factories are a useful way for thinking about bootstrapping because instead of thinking about a single bootstrap (you always need more than one!), you than think about a bootstrap __generator__, a function that yields a fresh boostrap every time it is called:

```{r}
boot_permute <- function(df, var) {
  n <- nrow(df)
  force(var)
  
  function() {
    df[[var]][sample(n, n, replace = TRUE)]
  }
}

boot_mtcars1 <- boot_permute(mtcars, "mpg")
head(boot_mtcars1())
head(boot_mtcars1())
```

The advantage of a function factory is more clear with a parametric bootstrap where we have to first fit a model. We can do this setup step once, when the factory is called, rather than once every time we generate the bootstrap:

```{r}
boot_model <- function(df, formula) {
  mod <- lm(formula, data = df)
  fitted <- unname(fitted(mod))
  resid <- unname(resid(mod))
  rm(mod)

  function() {
    fitted + sample(resid)
  }
} 

boot_mtcars2 <- boot_model(mtcars, mpg ~ wt)
head(boot_mtcars2())
head(boot_mtcars2())
```

I use `rm(mod)` because linear model objects are quite large (they include complete copies of the model matrix and input data) and I want to keep the manufactured function as small as possible.

### Maximum likelihood estimation {#MLE}
\index{maximum likelihood}
\indexc{optimise()}
\indexc{optim()}

The goal of maximum likelihood estimation (MLE) is to find the parameter values for a distribution that make the observed data "most likely". To do MLE, you start with a probability function. Let's work through a simple example, using the Poisson distribution. For example, if we know $\lambda$, we can compute the probability of getting a vector $\mathbf{x}$ of values ($x_1$, $x_2$, ..., $x_n$) by multiplying the Poisson probability function as follows:

\[ P(\lambda, \mathbf{x}) = \prod_{i=1}^{n} \frac{\lambda ^ {x_i} e^{-\lambda}}{x_i!} \]

In statistics, we almost always work with the log of this function. The log is a monotonic transformation which preserves important properties (i.e. the extrema occur in the same place), but has specific advantages:

* The log turns a product into a sum, which is easier to work with.

* Multiplying small numbers yields an even smaller number, which makes floating point math more challenging. Logged values are more numerically stable.

Let's log transform this probability function and simplify it as much as possible:

\[ \log(P(\lambda, \mathbf{x})) = \sum_{i=1}^{n} \log(\frac{\lambda ^ {x_i} e^{-\lambda}}{x_i!}) \]

\[ \log(P(\lambda, \mathbf{x})) = \sum_{i=1}^{n} \left( x_i \log(\lambda) - \lambda - \log(x_i!) \right) \]

\[ \log(P(\lambda, \mathbf{x})) = 
     \sum_{i=1}^{n} x_i \log(\lambda)
   - \sum_{i=1}^{n} \lambda 
   - \sum_{i=1}^{n} \log(x_i!) \]

\[ \log(P(\lambda, \mathbf{x})) = 
   \log(\lambda) \sum_{i=1}^{n} x_i - n \lambda - \sum_{i=1}^{n} \log(x_i!) \]

We can now turn this function into an R function. The R function is quite elegant because R is vectorised and, because it's a statistical programming language, R comes with built-in functions like the log-factorial (`lfactorial()`).

```{r}
lprob_poisson <- function(lambda, x) {
  n <- length(x)
  log(lambda) * sum(x) - n * lambda - sum(lfactorial(x))
}
```

Consider this vector of observations:

```{r}
x1 <- c(41, 30, 31, 38, 29, 24, 30, 29, 31, 38)
```

We can use `lprob_poisson()` to compute the (logged) probability of `x1` for different values of `lambda`. 

```{r}
lprob_poisson(10, x1)
lprob_poisson(20, x1)
lprob_poisson(30, x1)
```

The key idea of maximum likelihood is to think about the probability function in a different way. So far we've been thinking of `lambda` as fixed and known and the function tells us the probability of getting different `x` values. But in real-life, we observe the `x` and it is `lambda` that is unknown. The likelihood is the probability function, seen through this lens: we want to find the `lambda` that makes the observed `x` the "most likely". That is, given `x`, what value of `lambda` gives us the highest value of `lprob_poisson()`? 

In R, we can make this change in perspective more clear by using a function factory. We provide `x` and generate a function with a single parameter, `lambda`:

```{r}
ll_poisson <- function(x) {
  n <- length(x)

  function(lambda) {
    log(lambda) * sum(x) - n * lambda - sum(lfactorial(x))
  }
}
```

One nice thing about this approach is we can do some precomputation: any term that only involves `x` can be computed once in the factory. This is useful, because we're going to need to call this function many times to find the best `lambda`.

```{r}
ll_poisson <- function(x) {
  n <- length(x)
  sum_x <- sum(x)
  c <- sum(lfactorial(x))

  function(lambda) {
    log(lambda) * sum_x - n * lambda - c
  }
}
```

Now we can use this function to find the value of `lambda` that maximizes the (log) likelihood:

```{r}
ll1 <- ll_poisson(x1)

ll1(10)
ll1(20)
ll1(30)
```

Rather than trial and error, we can automate the process of finding the best value with `optimise()`. The results tell us that the highest value is `-30.27` which occurs when `lambda = 32.1`:

```{r}
optimise(ll1, c(0, 100), maximum = TRUE)
```

Now, we could have solved this problem without using a function factory because `optimise()` passes `...` on to the function being optimised. That means we could use the log-probability function directly:

```{r}
optimise(lprob_poisson, c(0, 100), x = x1, maximum = TRUE)$maximum
```

The advantage of using a function factory here is fairly small, but there are two niceties:

* We can precompute some values in the factory itself, saving computation time
  in each iteration.
  
* I think the two-level design better reflects the mathematical structure of 
  the underlying problem.

These advantages get bigger in more complex MLE problems, where you have multiple parameters and multiple data vectors.

### Exercises

1.  In `boot_model()`, why don't I need to force the evaluation of `df` 
    or `model`?
    
1.  Why might you formulate the Box-Cox transformation like this?

    ```{r}
    boxcox3 <- function(x) {
      function(lambda) {
        if (lambda == 0) {
          log(x)
        } else {
          (x ^ lambda - 1) / lambda
        }
      }  
    }
    ```

1.  Create a function that creates functions that compute the ith 
    [central moment](http://en.wikipedia.org/wiki/Central_moment) of a numeric 
    vector. You can test it by running the following code:

    ```{r, eval = FALSE}
    m1 <- moment(1)
    m2 <- moment(2)

    x <- runif(100)
    stopifnot(all.equal(m1(x), 0))
    stopifnot(all.equal(m2(x), var(x) * 99 / 100))
    ```

1.  Why don't you need to worry that `boot_permute()` stores a copy of the 
    data inside the function that it generates?

1.  Things are slightly less elegant when we generalise to more parameters 
    because `optim()`, the n-d generalisation of `optimise()`, calls the 
    function with a single argument containing a vector of parameters.

    ```{r}
    nll_normal <- function(x) {
      n <- length(x)
      
      function(params) {
        mu <- params[[1]]
        sigma <- params[[2]]
        
        n * log(sigma) + sum((x - mu) ^ 2) / (2 * sigma ^ 2)
      }
    }
    
    x3 <- c(10.1, 6.12, 8.48, 6.07, 5.27, 5.06, 6.51, 4.34, 3.68, 5.48)
    nll3 <- nll_normal(x1)
    optim(c(0, 1), nll3)$par
    ```

## Function factories + functionals {#functional-factories}

To finish off the chapter, I'll show how you might combine functions and function factories to produce a bunch of functions at once. The following code creates many specially named power functions by iterating over a list of arguments:

```{r}
names <- list(
  square = 2, 
  cube = 3, 
  root = 1/2, 
  cuberoot = 1/3, 
  reciprocal = -1
)
funs <- purrr::map(names, power1)

funs$root(64)
funs$root
```

They're currently trapped inside the list, but it's easier to assign into the environment of your choice:

```{r}
env_bind(globalenv(), !!!funs)

reciprocal(2)
```

This idea extends in a straightforward way your function factory takes two (`map2()`) or more (`pmap()`) argmunts.

You'll learn an alternative approach to the same problem in Section \@ref(quasi-function). Instead of using a function factory, you could construct the function with quasiquotation. This requires additional knowledge, but has the advantage of generating functions with more readable bodies, and avoids accidentally capturing large objects in the enclosing scope. The following code is a quick preview of how we could rewrite `power1()` to use quasiquotation 

```{r}
power3 <- function(exponent) {
  new_function(
    exprs(x = ), 
    expr({
      x ^ !!exponent
    }), 
    caller_env()
  )
}
funs <- purrr::map(names, power3)

funs$root
```

As well as `0.5` appearing as is, note that the environment of the function is the global environment, not an execution environment of `power3()`.
